{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bR4O8vRDKy1C"
   },
   "source": [
    "# Natural Language Processing \n",
    "### CS-UH 2216 - Spring 2019\n",
    "## Sentiment Analysis of 100,000 IMDB movie reviews\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Imdb movie review data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/README     aclImdb/imdb.vocab aclImdb/imdbEr.txt\r\n",
      "\r\n",
      "aclImdb/test:\r\n",
      "labeledBow.feat \u001b[34mneg\u001b[m\u001b[m             \u001b[34mpos\u001b[m\u001b[m             urls_neg.txt    urls_pos.txt\r\n",
      "\r\n",
      "aclImdb/train:\r\n",
      "labeledBow.feat \u001b[34mpos\u001b[m\u001b[m             unsupBow.feat   urls_pos.txt\r\n",
      "\u001b[34mneg\u001b[m\u001b[m             \u001b[34munsup\u001b[m\u001b[m           urls_neg.txt    urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "\n",
    "# The code below will check to see if the data directory exists; if not, it will download the data.\n",
    "if os.path.exists(\"./aclImdb\") == False:\n",
    "    print(\"Downloading the Imdb movie review data set\")\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar xf aclImdb_v1.tar.gz\n",
    "#Shell command to show the files and directories we have under aclImdb\n",
    "!ls aclImdb/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive training examples:\n",
      "12500\n",
      "number of negative training examples:\n",
      "12500\n",
      "number of unlabelled training examples:\n",
      "50000\n",
      "number of positive testing examples:\n",
      "12500\n",
      "number of negative testing examples:\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "#Run this cell first\n",
    "print(\"number of positive training examples:\")\n",
    "pos_train = !ls aclImdb/train/pos/ | wc -l\n",
    "pos_train = int(pos_train[0])\n",
    "print(pos_train)\n",
    "print(\"number of negative training examples:\")\n",
    "neg_train = !ls aclImdb/train/neg/ | wc -l\n",
    "neg_train = int(neg_train[0])\n",
    "print(neg_train)\n",
    "print(\"number of unlabelled training examples:\")\n",
    "unl_train = !ls aclImdb/train/unsup/ | wc -l\n",
    "unl_train = int(unl_train[0])\n",
    "print(unl_train)\n",
    "print(\"number of positive testing examples:\")\n",
    "pos_test = !ls aclImdb/test/pos/ | wc -l\n",
    "pos_test = int(pos_test[0])\n",
    "print(pos_test)\n",
    "print(\"number of negative testing examples:\")\n",
    "neg_test = !ls aclImdb/test/pos/ | wc -l\n",
    "neg_test = int(neg_test[0])\n",
    "print(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 How many reviews are for training? 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training reviews:\n",
      "75000\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of training reviews:\" )\n",
    "print(pos_train + neg_train + unl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 How many reviews are for testing? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training reviews:\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of training reviews:\" )\n",
    "print(pos_test + neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 How many reviews are positive (in total in training and testing)? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive instances in training and testing\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total positive instances in training and testing\" )\n",
    "print(pos_train + pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 How many reviews are negative (in total in training and testing)? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total negative instances in training and testing\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total negative instances in training and testing\" )\n",
    "print(neg_train + neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 How many reviews are unlabelled (in total in training and testing)? 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total negative instances in training and testing\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(\"total negative instances in training and testing\" )\n",
    "print(unl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What can we use unlabeled reviews for? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the unlabeled reviews for building unsupervised learning classification algorithms that can cluster and learn labels of reviews without being given the rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 How was the positive/negative labeling done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive and negative labels are classified based on the rating of the user reviews. A negative class is given to ratings <= 4 and a positive class is given to ratings >= 7 out of 10. Hence, reviews with more neural ratings are not included in the positive/negative sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Simply based on the labeling approach, do we expect some reviews to be harder than others for sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. This is because some reviews with the same label(pos/neg class) are closer to the extreme ends of the scale (1 ratings) while other reviews are closer to the neural end (4 rating). We would expect reviews with the same label but closer to the extreme end of the scale, to be easier to analyze because they likely have more obvious features which we can use for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 How many are the most negative review [1] (train and test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of most negative reviews\n",
      "10122\n"
     ]
    }
   ],
   "source": [
    "max_neg_test = !ls aclImdb/test/neg/ | grep \"\\w*1.txt\" | wc -l\n",
    "max_neg_test = int(max_neg_test[0])\n",
    "max_neg_train = !ls aclImdb/train/neg/ | grep \"\\w*1.txt\" | wc -l\n",
    "max_neg_train = int(max_neg_train[0])\n",
    "print(\"Total number of most negative reviews\")\n",
    "print(max_neg_test + max_neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 How many are the most positive reviews [10] (train and test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of most positive reviews\n",
      "9731\n"
     ]
    }
   ],
   "source": [
    "max_pos_test = !ls aclImdb/test/pos/ | grep \"\\w*10.txt\" | wc -l\n",
    "max_pos_test = int(max_pos_test[0])\n",
    "max_pos_train = !ls aclImdb/train/pos/ | grep \"\\w*10.txt\" | wc -l\n",
    "max_pos_train = int(max_pos_train[0])\n",
    "print(\"Total number of most positive reviews\")\n",
    "print(max_pos_test + max_pos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files #load_files load text files with categories as subfolder names; \n",
    "\n",
    "# Directory of our data\n",
    "traindir = r'./aclImdb/train'\n",
    "testdir = r'./aclImdb/test'\n",
    "\n",
    "# load pos/neg train and test data\n",
    "train=load_files(traindir,categories=['pos','neg']) #load_files shuffles the text and categories by default.\n",
    "test=load_files(testdir,categories=['pos','neg'])\n",
    "\n",
    "# load an object with all the training data (positive, negative and unlabeled)\n",
    "alltrain = load_files(traindir,categories=['pos','neg','unsup'])\n",
    "#load_files return a dictionary-like object:\n",
    "#1. 'data': the raw text data to learn\n",
    "#2. 'target': the classification labels (integer index)\n",
    "#3. 'target_names': the meaning of the labels\n",
    "#4. 'filenames': the name of the file holding the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  = 13374\n",
      "\n",
      "Text = b'Since this movie was based on a true story of a woman who had two children and was not very well-off, it was just scary as to how real it really was! The acting is what gave the movie that push to greatness.<br /><br />Diane Keaton portrayed the main character, Patsy McCartle who had two sons whom she adored. Her performance is what made the real life story come to life on a television screen. It was very hard to watch some of the scenes since they were so real as to what happens when one becomes addicted to drugs.<br /><br />Just watching this very loving mother go from sweet to not caring at all was hard, but so true. I have known people who have gone through withdrawl and it was very much like what happened in this movie, from what I remember.<br /><br />I also thought that it was very risky for the director to want to make a movie out of what happened to this woman. Yet it was done so well. I applaud the director for making this movie.<br /><br />I highly recommend this to anyone who has known someone who has ever been addicted to drugs or to just learn what can happen to you if you do become addicted to them.'\n",
      "\n",
      "Label = pos\n",
      "\n",
      "Filename = ./aclImdb/train/pos/1953_10.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Browse an example\n",
    "#For example, if we want to see data point with index i \n",
    "i = 13374\n",
    "print(\"Index  = %3d\\n\" % (i))\n",
    "print(\"Text = %s\\n\" % (train.data[i]))\n",
    "print(\"Label = %s\\n\" %(train.target_names[train.target[i]]))\n",
    "print(\"Filename = %s\\n\" % (train.filenames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Investigating the quality of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine rating given the review filename\n",
    "import re\n",
    "def getRating(filename):\n",
    "    match = re.search(\"_(.+).txt\",filename)\n",
    "    if match: #If we have found something\n",
    "        return(int(match.group(1))) #Return the first capture group ()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_print_rating(rating):\n",
    "    #listing filename, label, and at least the first 10 words of text\n",
    "    for i in range(len(train.filenames)):\n",
    "        if(getRating(train.filenames[i]) == rating):\n",
    "            print(\"Label = %s\" %(train.target_names[train.target[i]]))\n",
    "            print(\"Filename: \", train.filenames[i])\n",
    "            print(\"Rating: \", getRating(train.filenames[i]))\n",
    "            print(\"Text: \", train.data[i][0:200])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identify an example of a strong negative sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_print_rating(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Identify an example of a weak negative sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_print_rating(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Identify an example of a strong positive sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_print_rating(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Identify an example of a weak positive sentiment based on human ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_print_rating(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 List one observation of a feature of the text that you think will be helpful to predict sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the choice of vocabulary seems to be correlated with the rating of the review. Certain vocabulary such as \"Horrible\" are seen significantly more frequently in negative instances than in positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of reviews with \"Horrible\" in contents.\n",
    "negativeOccurances = 0\n",
    "positiveOccurances = 0\n",
    "for i in range(len(train.filenames)):\n",
    "    if(train.target_names[train.target[i]] == 'pos'):\n",
    "        if 'horrible' in str(train.data[i]):\n",
    "            positiveOccurances += 1\n",
    "    else:\n",
    "        if 'horrible' in str(train.data[i]):\n",
    "            negativeOccurances += 1\n",
    "print(\"Number of times \\\"horrible\\\" appeared in negative reviews: \", negativeOccurances)\n",
    "print(\"Number of times \\\"horrible\\\" appeared in positive reviews: \", positiveOccurances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a basic sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #Vectorize our text with top 1000 frequent vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vectorizer and transform our data\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True) #Tokenize at word level\n",
    "vectorizer.fit(alltrain.data)\n",
    "train_data_vectorized = vectorizer.transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a logistic linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_data_vectorized, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "test_pred = model.predict(vectorizer.transform(test.data))\n",
    "#Preliminary evaluation with sklearn\n",
    "print (\"Accuracy  = %3.2f%%\" % (100*accuracy_score(test.target, test_pred)))\n",
    "print (\"Average Precision = %3.2f%%\" % (100*precision_score(test.target, test_pred, average='macro')))\n",
    "print (\"Average Recall    = %3.2f%%\" % (100*recall_score(test.target, test_pred, average='macro')))\n",
    "print (\"Average F1-score  = %3.2f%%\" % (100*f1_score(test.target, test_pred, average='macro')))\n",
    "# The scores for accuracy, and average precision, recall and F1-score are similar. \n",
    "# This is in part an effect of the balanced nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluating Accuracy, Recall and F1 scores without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record the instances of correct and incorrect predictions\n",
    "num_neg = 0\n",
    "num_pos = 0\n",
    "neg_correct = 0\n",
    "neg_incorrect = 0\n",
    "pos_correct = 0\n",
    "pos_incorrect = 0\n",
    "\n",
    "for i in range(len(test.target)):\n",
    "    if test.target[i] == 0:\n",
    "        num_neg +=1\n",
    "        if test_pred[i] == 0:\n",
    "            neg_correct += 1\n",
    "        elif test_pred[i] == 1:\n",
    "            neg_incorrect += 1\n",
    "    elif test.target[i] == 1:\n",
    "        num_pos += 1\n",
    "        if test_pred[i] == 0:\n",
    "            pos_incorrect += 1\n",
    "        elif test_pred[i] == 1:\n",
    "            pos_correct += 1\n",
    "\n",
    "print(\"Total instances %d\" % (num_neg + num_pos))\n",
    "print(\"Total correct predictions %d\" % (pos_correct + neg_correct) )\n",
    "print(\"Correctly predicted negative %d\" % (neg_correct))\n",
    "print(\"Incorrectly predicted negative %d\" % (neg_incorrect))\n",
    "print(\"Correctly predicted positive %d\" % (pos_correct))\n",
    "print(\"Incorrectly predicted positive %d\" % (pos_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy = Correctly predicted / Total\n",
    "Accuracy = (pos_correct + neg_correct)/(num_neg + num_pos)\n",
    "print(\"Accuracy: %2.2f%%\" % (100*Accuracy))\n",
    "print()\n",
    "\n",
    "#Neg label precision = neg_correct / (neg_correct + pos_incorrect)\n",
    "neg_precision = neg_correct / (neg_correct + pos_incorrect)\n",
    "print(\"Neg label precision: %2.2f%%\" % (100*neg_precision))\n",
    "\n",
    "#Recall of Neg = neg_correct / num_neg\n",
    "neg_recall = neg_correct / num_neg\n",
    "print(\"Neg label recall: %2.2f%%\" % (100*neg_recall))\n",
    "\n",
    "#F1-score harmonic mean of precision and recall. = \n",
    "neg_f1 = 1/( ((1/neg_precision)+(1/neg_recall))/2 )\n",
    "print(\"Neg F1 score: %2.2f%%\" % (100*neg_f1))\n",
    "print()\n",
    "\n",
    "#Pos label precision = pos_correct / (pos_correct + neg_incorrect)\n",
    "pos_precision = pos_correct / (pos_correct + neg_incorrect)\n",
    "print(\"Pos label precision: %2.2f%%\" % (100*pos_precision))\n",
    "\n",
    "#Recall of Pos = pos_correct / num_pos\n",
    "pos_recall = pos_correct / num_pos\n",
    "print(\"Pos label recall: %2.2f%%\" % (100*pos_recall))\n",
    "\n",
    "#F1-score harmonic mean of precision and recall. = \n",
    "pos_f1 = 1/( ((1/pos_precision)+(1/pos_recall))/2 )\n",
    "print(\"Pos F1 score: %2.2f%%\" % (100*pos_f1))\n",
    "print()\n",
    "\n",
    "print(\"Average precision %2.2f\" % (100*(neg_precision + pos_precision)/2))\n",
    "print(\"Average recall %2.2f\" % (100*(neg_recall + pos_recall)/2))\n",
    "print(\"Average F1 score %2.2f%%\" % (100*(neg_f1 + pos_f1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Individual accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Accuracy for each rating (1-4, 7-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingCorrect = dict()\n",
    "ratingIncorrect = dict()\n",
    "ratings = [1,2,3,4,7,8,9,10]\n",
    "#Initialize the ratings\n",
    "for rating in ratings:\n",
    "    ratingCorrect[rating] = 0\n",
    "    ratingIncorrect[rating] = 0\n",
    "    \n",
    "#Count correct and incorrect instances\n",
    "for i in range(len(test.target)):\n",
    "    rating = getRating(test.filenames[i])\n",
    "    if test.target[i] == test_pred[i]:\n",
    "        ratingCorrect[rating] += 1\n",
    "    else:\n",
    "        ratingIncorrect[rating] += 1\n",
    "\n",
    "# Print the accuracy.\n",
    "for rating in ratings:\n",
    "    print(\"Rating %d accuracy: %2.2f\" % (rating, 100*ratingCorrect[rating]/(ratingCorrect[rating]+ratingIncorrect[rating])))\n",
    "    if rating == 4:\n",
    "        print() #Print a space betwen \"positive\" and \"negative\" ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our performance increases near the extreme for the negative class (1) and the extreme for the positive class (10). The performance decreases when the ratings are more neural and close to the center of the scale. This was predicted earlier in § 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data samples for each instance in the cofusion matrix (TP FP FN TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze instances of TN(Neg Neg) FP (Neg Pos) FN(Pos Neg) and TP (Pos Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInstance(gold, pred):\n",
    "    for i in range(len(test.target)):\n",
    "        if test.target[i] == gold and test_pred[i] == pred:\n",
    "            return(i)\n",
    "    return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True negative\")\n",
    "instance = getInstance(0,0)\n",
    "print(\"Gold: Neg | Predicted: Neg | Match: Correct | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This negative example was correctly predicted negatively. Since we are analyzing word frequency, this review has a lot of words such as \"dissapointed\" \"poor\" \"attempt\" and \"wasted\" which would have likely pushed the algorithm to predict a negative classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False positive\")\n",
    "instance = getInstance(0,1)\n",
    "print(\"Gold: Neg | Predicted: Pos | Match: Incorrect | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was predicted to be positive when it was negative. First this example was a difficult review to classify as it's rating was quite near the center (4). Second, the review uses a lot of vocabulary such as \"love\" \"pleasant\" \"charm\" frequently which indicate that the reviewed liked the movie. The model doesn't know that the reviewer could be describing the contents of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False negative\")\n",
    "instance = getInstance(1,0)\n",
    "print(\"Gold: Pos | Predicted: Neg | Match: Incorrect | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the above review, this review was also close to the center of the scale. In addition, even as a human reading the review, it would be difficult to predict the correct rating. The words used in this review doesn't indicate clearly that it is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True positive\")\n",
    "instance = getInstance(1,1)\n",
    "print(\"Gold: Pos | Predicted: Pos | Match: Correct | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was correctly predicted positive. It's likely because of the positive words used \"fun\", \"work well\" and \"congratulations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Investigating the effect of training size on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increment the training size from 1000 to 25000 in increments of 1000 and examine the increase in accuracy as we increase the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True) #Tokenize at word level\n",
    "vectorizer.fit(alltrain.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model with varying sizes of train data\n",
    "F1list = [] #Track the F1 scores\n",
    "trainSize = [] #Track our training size\n",
    "for i in range(1,26):\n",
    "    trainSize.append(i*1000)\n",
    "    #Create a model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    #Train with our dataset\n",
    "    model.fit(vectorizer.transform(train.data[0:i*1000]), train.target[0:i*1000])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print (\"Training size: %d Average F1-score: %3.2f%%\" % (i*1000, F1))\n",
    "    F1list.append(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Function to plot trainsize vs Fscore\n",
    "def plotTrainsizeFscore(trainSize,F1list):\n",
    "    plt.figure(figsize=(8, 4)) \n",
    "    plt.plot(trainSize,F1list,color='blue')\n",
    "    plt.xlabel('Training size', fontsize=10)\n",
    "    plt.ylabel('F-score %', fontsize=10)\n",
    "    plt.title('Learning Curve',fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot our F1 against data Size\n",
    "plotTrainsizeFscore(trainSize,F1list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observe that our logistic regression model was suprisingly accurate even when training even with a minimal of 1000 samples. It seems that given only 1000 samples, our model could identify the sentiment relatively well\n",
    "\n",
    "I observe that as the training size increases, the F1 score of the model increases. For example, with only 1000 training samples we can achieve an accuracy of 77.6% while with 5,000 training samples we can achieve an accuracy of 81.8%. This is to be expected because the more training examples we have, the better the predicting capability and generalization to the unseen test the model is.\n",
    "\n",
    "I also observe that as the training size increases, the rate of gain in F1 of adding on an additional 1000 sample decreased. Simply put the F1 score did not increase linearlly to the size of the Training. There appears to be a log relationship.\n",
    "\n",
    "Between 1000 and 2000 examples, the F score increased 1.65% Between 2000 and 4000 it increased 1.34%. Between 4000 and 8000 it increased 3.15%. Between 8000 and 16000 it increased 1.88%. \n",
    "\n",
    "Following this observed relationship, I would expect an increase of around 2% for every doubling in training size. Hence, I expect an increase of another 2% to 88% if we add another 25000 examples. (Although this is not guranteed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Investigating the effect of featureSize and trainingSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training size varies in increments of 5000 while our feature size (words) varies between 100, 1000 and 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for featureSize in [100, 1000, 10000]:\n",
    "    print(\"Vectorzing with %d features\" % (featureSize))\n",
    "    #Create the vectorizer\n",
    "    vectorizer = CountVectorizer(max_features=featureSize,analyzer='word',lowercase=True) #Tokenize at word level\n",
    "    vectorizer.fit(alltrain.data)\n",
    "\n",
    "    #Train the model with varying sizes of train data\n",
    "    F1list = [] #Track the F1 scores\n",
    "    trainSize = [] #Track our training size\n",
    "    for i in [1,5,10,15,20,25]:\n",
    "        trainSize.append(i*1000)\n",
    "        #Create a model\n",
    "        model = LogisticRegression(solver='liblinear')\n",
    "        #Train with our dataset\n",
    "        model.fit(vectorizer.transform(train.data[0:i*1000]), train.target[0:i*1000])\n",
    "        #Vectorize our test data.\n",
    "        test_pred = model.predict(vectorizer.transform(test.data))\n",
    "        #Determine our F1 score\n",
    "        F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "        print (\"Training size: %d Average F1-score: %3.2f%%\" % (i*1000, F1))\n",
    "        F1list.append(F1)\n",
    "        \n",
    "    #Plot our F1 against data Size\n",
    "    print(\"Learning curve plot vectorized with %d\" % (featureSize))\n",
    "    plotTrainsizeFscore(trainSize,F1list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an interesting investigation. We learnt from §5 that as the training size increases, the F-score increases (although non-linearly). In this section, we conducted the same training size variation, but using different levels of features (words) namely features of 100, 1000 and 10000. What I observed was that not only did the F1-score change, but the profile of the training curve also changed with different number of features.\n",
    "\n",
    "First, using only 100 word features, we see that the F1 score at with a training_size of 1000 was the lowest at 69.86%. With 1000 word features, this increased to 77.57% and with 10,000 this climbed to 79.78%. It seems that as the number of word features increase, the F-score also increases. \n",
    "\n",
    "However, this holds less weight when we train with the full training_size of 25000. At 100 word features, the F1 score was 73.22% and at 1000 features increased to 85.95% as expected. However, with 10,000 features, the score actually decreased to 85.52%.This could suggest that there are only a certain number of \"indicator\" words utilized by our model. After around 1000 features, most of the additional features are perhaps not useful to our predictor. In addition having more features could result in overfitting of our model. This means that after training, our model will generalize less well to the unseen test set.\n",
    "\n",
    "The learning curve profile also varied between different feature settings. For instance, with 100 features, the model quickly learnt the most optimal model. After 5000-10000 training examples, the model did not imrpove the score. It suggests that maybe the model learnt the most optimal parameters. Furthurmore, this score actually dropped after 20,000 instances suggesting either overfitting or lack of features to search over since the feature space is so constrained.\n",
    "\n",
    "With 1000 features, the model could continually and gradually improve its score with added training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Investigating the effect of TfidVectorizer, and MultinomialNaiveBayes model and training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing our vectorizer and model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer and LogisticRegression\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True)\n",
    "vectorizer.fit(alltrain.data)\n",
    "print(\"Vectorized with CountVectorizer and trained with LogisticRegression algorithm\")\n",
    "for trainSize in [1000, 5000, 25000]:\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(vectorizer.transform(train.data[0:trainSize]), train.target[0:trainSize])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print(\"Training size: %d Average F1-score: %3.2f%%\" % (trainSize, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer and MultinomialNaiveBayes\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True)\n",
    "vectorizer.fit(alltrain.data)\n",
    "print(\"Vectorized with CountVectorizer and trained with MultinomialNaiveBayes algorithm\")\n",
    "for trainSize in [1000, 5000, 25000]:\n",
    "    model = MultinomialNB()\n",
    "    model.fit(vectorizer.transform(train.data[0:trainSize]), train.target[0:trainSize])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print(\"Training size: %d Average F1-score: %3.2f%%\" % (trainSize, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidVectorizer and MultinomialNaiveBayes\n",
    "vectorizer = TfidfVectorizer(max_features=1000,analyzer='word',lowercase=True)\n",
    "vectorizer.fit(alltrain.data)\n",
    "print(\"Vectorized with TfidVectorizer and trained with MultinomialNaiveBayes algorithm\")\n",
    "for trainSize in [1000, 5000, 25000]:\n",
    "    model = MultinomialNB()\n",
    "    model.fit(vectorizer.transform(train.data[0:trainSize]), train.target[0:trainSize])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print(\"Training size: %d Average F1-score: %3.2f%%\" % (trainSize, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidVectorizer and LogisticRegression\n",
    "vectorizer = TfidfVectorizer(max_features=1000,analyzer='word',lowercase=True)\n",
    "vectorizer.fit(alltrain.data)\n",
    "print(\"Vectorized with TfidVectorizer and trained with LogisticRegression\")\n",
    "for trainSize in [1000, 5000, 25000]:\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(vectorizer.transform(train.data[0:trainSize]), train.target[0:trainSize])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print(\"Training size: %d Average F1-score: %3.2f%%\" % (trainSize, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first setting of note that applies across the different scales is the size of the training data. In these scenarios, as the size of the training data increases, so does the performance. \n",
    "\n",
    "However, certain setting combinations (vectorizer and model) gain a higher performance increase from an increase in the size of training. The most significant is perhaps using CountVectorizer and Logistic regression. In this combination, the performance increased from 77.6% to 86% when we used the full training data (1000 to 25000). The least significant is using CountVectorizer and MultinomialNaiveBayes algorithm which only resulted in an performance increase from 79.3% to 81% when the train data size was increased from 1000 to 25000.\n",
    "\n",
    "Using MultinomialNaiveBayes seems to boost the performance (when compared to LogisticRegression) in all cases when a low training size is used (1000), but decrease the performance when more training data is given (5000+). Using TfidVectorizer also seems to increas the performance (when compared to CountVectorizer) for both training algorithms and across all scales of data. \n",
    "\n",
    "The best performing setting when given a low amount of training data (1000) was using TfidVectorizer with MultinomialNaiveBayes with a score of 81.23% while the worst was CountVectorizer and LogisticRegression that resulted in a score of 77.6%. This situation was negated when we used the full training set (25000). In this case the best performing algorithm was TfidVectorizer with LogisticRegression while the worst was CountVectorizer and MultinomialNaiveBayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from the above observations, I would elect to use different settings depending on the amount of training data I have. If I had a small amount of training data (1000) I would use (TfidVectorizer and MultinomialNaiveBayes). If I had a higher amount of training data (5000+) I would use (TfidVectorizer and LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Building a Word2Vec model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Gensim package implementation of Word2Vec \n",
    "import gensim\n",
    "# Import Word2Vec from Gensim\n",
    "from gensim.models import Word2Vec\n",
    "#Use the same tokenization strategy as in previous sections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "wordtokenizer=CountVectorizer().build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min, sys: 2.05 s, total: 4min 2s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "#A Word2Vec model taking in array of vectorized words\n",
    "#Using the following settings: word vector size=150, model window=10, min_count of words to include=2\n",
    "%time wv_model = gensim.models.Word2Vec([wordtokenizer(str(i)) for i in alltrain.data],size=150,window=10,min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of our learnt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word = Queen\n",
      "Token = queen\n",
      "Most Similar:\n",
      "[('princess', 0.7151473164558411), ('maid', 0.6598435044288635), ('victoria', 0.6352784633636475)]\n",
      "------------\n",
      "Word Vector(first 10):\n",
      "[-2.7750227   1.8705103  -0.76944304 -1.0814235  -2.808338   -0.8339399\n",
      " -0.62903166 -1.432056   -1.7692068  -0.44407073]\n",
      "Word Vector length:\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "#Given a word, get the top 3 similar words and display the interal word vector.\n",
    "word = \"Queen\"\n",
    "#Tokenize word, taking the first token in the list\n",
    "tokword=wordtokenizer(word)[0]\n",
    "\n",
    "print(\"Word = %s\" % word)\n",
    "print(\"Token = %s\" % tokword)\n",
    "\n",
    "#Proceed if word is in the model\n",
    "if (tokword in wv_model.wv.vocab):\n",
    "    print(\"Most Similar:\")\n",
    "    print(wv_model.wv.most_similar(tokword,topn=3))\n",
    "    print(\"------------\")\n",
    "    print(\"Word Vector(first 10):\")\n",
    "    print(wv_model.wv[tokword][0:10])\n",
    "    print(\"Word Vector length:\")\n",
    "    print(len(wv_model.wv[tokword]))\n",
    "else:\n",
    "    print(\"Unknown Word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france - paris + berlin = \n",
      "[('germany', 0.742906928062439), ('spain', 0.724894642829895), ('wwi', 0.6913067698478699)]\n"
     ]
    }
   ],
   "source": [
    "#Analogies: A1 to B1 is like what to B2 <br> (A1 - B1 + B2 = A2) <br> *e.g. King - Man + Woman = Queen*\n",
    "A1_word = \"France\" \n",
    "B1_word = \"Paris\"\n",
    "B2_word = \"Berlin\"\n",
    "\n",
    "#Tokenize words\n",
    "A1_tokword=wordtokenizer(A1_word)[0]\n",
    "B1_tokword=wordtokenizer(B1_word)[0]\n",
    "B2_tokword=wordtokenizer(B2_word)[0]\n",
    "\n",
    "#Analogy formula\n",
    "print(\"%s - %s + %s = \" % (A1_tokword,B1_tokword,B2_tokword))\n",
    "\n",
    "#Check if our word is in the vocab of our model\n",
    "if (A1_tokword in wv_model.wv.vocab and B1_tokword in wv_model.wv.vocab and B2_tokword in wv_model.wv.vocab):\n",
    "    print(wv_model.wv.most_similar(positive=[A1_tokword,B2_tokword],negative=[B1_tokword],topn=3))\n",
    "else:\n",
    "    print(\"One of the given words is not known\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: the young king told us he was angry | the prince said he was mad\n",
      "Sentence Similarity 74.226\n",
      "Sentences: twinkle twinkle little star | what is the meaning of life\n",
      "Sentence Similarity 0.591\n"
     ]
    }
   ],
   "source": [
    "#Sentence Similarty:  sentence1 to sentence2 \n",
    "#Word2Vec is for words so to evaluate sentences, we map words into a single vector\n",
    "\n",
    "#Map senteneces to a single vecvtor. Tokenize and filter unknown words.\n",
    "def meanW2VTransform (sent):\n",
    "    return np.mean(\n",
    "        [wv_model.wv[wordi] for wordi in list(filter(lambda i : i in wv_model.wv.vocab, wordtokenizer(str(sent))))],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "#Use cosine similarity to measure the distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "\n",
    "sentence1 = \"the young king told us he was angry\"\n",
    "sentence2 = \"the prince said he was mad\"\n",
    "print(\"Sentences: %s | %s\" % (sentence1, sentence2))\n",
    "print(\"Sentence Similarity %3.3f\" %(100*cosine([meanW2VTransform(sentence1)],[meanW2VTransform(sentence2)])))\n",
    "\n",
    "sentence1 = \"twinkle twinkle little star\"\n",
    "sentence2 = \"what is the meaning of life\"\n",
    "print(\"Sentences: %s | %s\" % (sentence1, sentence2))\n",
    "print(\"Sentence Similarity %3.3f\" %(100*cosine([meanW2VTransform(sentence1)],[meanW2VTransform(sentence2)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5DNa04kjrPf"
   },
   "source": [
    "#### Sentence-level sentiment analysis using a vectorizer based on average vectors of words in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
