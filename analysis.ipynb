{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bR4O8vRDKy1C"
   },
   "source": [
    "# Natural Language Processing \n",
    "### CS-UH 2216 - Spring 2019\n",
    "## Sentiment Analysis of 100,000 IMDB movie reviews\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Imdb movie review data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/imdbEr.txt  aclImdb/imdb.vocab\taclImdb/README\r\n",
      "\r\n",
      "aclImdb/test:\r\n",
      "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\r\n",
      "\r\n",
      "aclImdb/train:\r\n",
      "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\r\n",
      "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\r\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "\n",
    "# The code below will check to see if the data directory exists; if not, it will download the data.\n",
    "if os.path.exists(\"./aclImdb\") == False:\n",
    "    print(\"Downloading the Imdb movie review data set\")\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !tar xf aclImdb_v1.tar.gz\n",
    "#Shell command to show the files and directories we have under aclImdb\n",
    "!ls aclImdb/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive training examples:\n",
      "12500\n",
      "number of negative training examples:\n",
      "12500\n",
      "number of unlabelled training examples:\n",
      "50000\n",
      "number of positive testing examples:\n",
      "12500\n",
      "number of negative testing examples:\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "#Run this cell first\n",
    "print(\"number of positive training examples:\")\n",
    "pos_train = !ls aclImdb/train/pos/ | wc -l\n",
    "pos_train = int(pos_train[0])\n",
    "print(pos_train)\n",
    "print(\"number of negative training examples:\")\n",
    "neg_train = !ls aclImdb/train/neg/ | wc -l\n",
    "neg_train = int(neg_train[0])\n",
    "print(neg_train)\n",
    "print(\"number of unlabelled training examples:\")\n",
    "unl_train = !ls aclImdb/train/unsup/ | wc -l\n",
    "unl_train = int(unl_train[0])\n",
    "print(unl_train)\n",
    "print(\"number of positive testing examples:\")\n",
    "pos_test = !ls aclImdb/test/pos/ | wc -l\n",
    "pos_test = int(pos_test[0])\n",
    "print(pos_test)\n",
    "print(\"number of negative testing examples:\")\n",
    "neg_test = !ls aclImdb/test/pos/ | wc -l\n",
    "neg_test = int(neg_test[0])\n",
    "print(neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 How many reviews are for training? 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training reviews:\n",
      "75000\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of training reviews:\" )\n",
    "print(pos_train + neg_train + unl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 How many reviews are for testing? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of training reviews:\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of training reviews:\" )\n",
    "print(pos_test + neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 How many reviews are positive (in total in training and testing)? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total positive instances in training and testing\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total positive instances in training and testing\" )\n",
    "print(pos_train + pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 How many reviews are negative (in total in training and testing)? 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total negative instances in training and testing\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(\"total negative instances in training and testing\" )\n",
    "print(neg_train + neg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 How many reviews are unlabelled (in total in training and testing)? 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total negative instances in training and testing\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(\"total negative instances in training and testing\" )\n",
    "print(unl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 What can we use unlabeled reviews for? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the unlabeled reviews for building unsupervised learning classification algorithms that can cluster and learn labels of reviews without being given the rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 How was the positive/negative labeling done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive and negative labels are classified based on the rating of the user reviews. A negative class is given to ratings <= 4 and a positive class is given to ratings >= 7 out of 10. Hence, reviews with more neural ratings are not included in the positive/negative sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Simply based on the labeling approach, do we expect some reviews to be harder than others for sentiment analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. This is because some reviews with the same label(pos/neg class) are closer to the extreme ends of the scale (1 ratings) while other reviews are closer to the neural end (4 rating). We would expect reviews with the same label but closer to the extreme end of the scale, to be easier to analyze because they likely have more obvious features which we can use for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 How many are the most negative review [1] (train and test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of most negative reviews\n",
      "10122\n"
     ]
    }
   ],
   "source": [
    "max_neg_test = !ls aclImdb/test/neg/ | grep \"\\w*1.txt\" | wc -l\n",
    "max_neg_test = int(max_neg_test[0])\n",
    "max_neg_train = !ls aclImdb/train/neg/ | grep \"\\w*1.txt\" | wc -l\n",
    "max_neg_train = int(max_neg_train[0])\n",
    "print(\"Total number of most negative reviews\")\n",
    "print(max_neg_test + max_neg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 How many are the most positive reviews [10] (train and test)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of most positive reviews\n",
      "9731\n"
     ]
    }
   ],
   "source": [
    "max_pos_test = !ls aclImdb/test/pos/ | grep \"\\w*10.txt\" | wc -l\n",
    "max_pos_test = int(max_pos_test[0])\n",
    "max_pos_train = !ls aclImdb/train/pos/ | grep \"\\w*10.txt\" | wc -l\n",
    "max_pos_train = int(max_pos_train[0])\n",
    "print(\"Total number of most positive reviews\")\n",
    "print(max_pos_test + max_pos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_files #load_files load text files with categories as subfolder names; \n",
    "\n",
    "# Directory of our data\n",
    "traindir = r'./aclImdb/train'\n",
    "testdir = r'./aclImdb/test'\n",
    "\n",
    "# load pos/neg train and test data\n",
    "train=load_files(traindir,categories=['pos','neg']) #load_files shuffles the text and categories by default.\n",
    "test=load_files(testdir,categories=['pos','neg'])\n",
    "\n",
    "# load an object with all the training data (positive, negative and unlabeled)\n",
    "alltrain = load_files(traindir,categories=['pos','neg','unsup'])\n",
    "#load_files return a dictionary-like object:\n",
    "#1. 'data': the raw text data to learn\n",
    "#2. 'target': the classification labels (integer index)\n",
    "#3. 'target_names': the meaning of the labels\n",
    "#4. 'filenames': the name of the file holding the data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index  = 13374\n",
      "\n",
      "Text = b'Since this movie was based on a true story of a woman who had two children and was not very well-off, it was just scary as to how real it really was! The acting is what gave the movie that push to greatness.<br /><br />Diane Keaton portrayed the main character, Patsy McCartle who had two sons whom she adored. Her performance is what made the real life story come to life on a television screen. It was very hard to watch some of the scenes since they were so real as to what happens when one becomes addicted to drugs.<br /><br />Just watching this very loving mother go from sweet to not caring at all was hard, but so true. I have known people who have gone through withdrawl and it was very much like what happened in this movie, from what I remember.<br /><br />I also thought that it was very risky for the director to want to make a movie out of what happened to this woman. Yet it was done so well. I applaud the director for making this movie.<br /><br />I highly recommend this to anyone who has known someone who has ever been addicted to drugs or to just learn what can happen to you if you do become addicted to them.'\n",
      "\n",
      "Label = pos\n",
      "\n",
      "Filename = ./aclImdb/train/pos/1953_10.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Browse an example\n",
    "#For example, if we want to see data point with index i \n",
    "i = 13374\n",
    "print(\"Index  = %3d\\n\" % (i))\n",
    "print(\"Text = %s\\n\" % (train.data[i]))\n",
    "print(\"Label = %s\\n\" %(train.target_names[train.target[i]]))\n",
    "print(\"Filename = %s\\n\" % (train.filenames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Investigating the quality of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine rating given the review filename\n",
    "import re\n",
    "def getRating(filename):\n",
    "    match = re.search(\"_(.+).txt\",filename)\n",
    "    if match: #If we have found something\n",
    "        return(int(match.group(1))) #Return the first capture group ()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_print_rating(rating):\n",
    "    #listing filename, label, and at least the first 10 words of text\n",
    "    for i in range(len(train.filenames)):\n",
    "        if(getRating(train.filenames[i]) == rating):\n",
    "            print(\"Label = %s\" %(train.target_names[train.target[i]]))\n",
    "            print(\"Filename: \", train.filenames[i])\n",
    "            print(\"Rating: \", getRating(train.filenames[i]))\n",
    "            print(\"Text: \", train.data[i][0:200])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identify an example of a strong negative sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = neg\n",
      "Filename:  ./aclImdb/train/neg/6802_1.txt\n",
      "Rating:  1\n",
      "Text:  b\"Words can't describe how bad this movie is. I can't explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do th\"\n"
     ]
    }
   ],
   "source": [
    "find_and_print_rating(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Identify an example of a weak negative sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = neg\n",
      "Filename:  ./aclImdb/train/neg/9503_4.txt\n",
      "Rating:  4\n",
      "Text:  b'Also known in a different form as \"House of Exorcism,\" this messy<br /><br />little film takes itself so seriously as to kill any entertainment value<br /><br />whatsoever.<br /><br />The spare plot i'\n"
     ]
    }
   ],
   "source": [
    "find_and_print_rating(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Identify an example of a strong positive sentiment based on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = pos\n",
      "Filename:  ./aclImdb/train/pos/11485_10.txt\n",
      "Rating:  10\n",
      "Text:  b'Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of be'\n"
     ]
    }
   ],
   "source": [
    "find_and_print_rating(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Identify an example of a weak positive sentiment based on human ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = pos\n",
      "Filename:  ./aclImdb/train/pos/9846_7.txt\n",
      "Rating:  7\n",
      "Text:  b'Ok, at the beginning it looked like \"Shrek\" - the loner that is persistently followed by the comic relief. Then it evolves into something really compelling, as the gauntlet is set. And the result is a'\n"
     ]
    }
   ],
   "source": [
    "find_and_print_rating(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 List one observation of a feature of the text that you think will be helpful to predict sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the choice of vocabulary seems to be correlated with the rating of the review. Certain vocabulary such as \"Horrible\" are seen significantly more frequently in negative instances than in positive instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times \"horrible\" appeared in negative reviews:  796\n",
      "Number of times \"horrible\" appeared in positive reviews:  150\n"
     ]
    }
   ],
   "source": [
    "#Number of reviews with \"Horrible\" in contents.\n",
    "negativeOccurances = 0\n",
    "positiveOccurances = 0\n",
    "for i in range(len(train.filenames)):\n",
    "    if(train.target_names[train.target[i]] == 'pos'):\n",
    "        if 'horrible' in str(train.data[i]):\n",
    "            positiveOccurances += 1\n",
    "    else:\n",
    "        if 'horrible' in str(train.data[i]):\n",
    "            negativeOccurances += 1\n",
    "print(\"Number of times \\\"horrible\\\" appeared in negative reviews: \", negativeOccurances)\n",
    "print(\"Number of times \\\"horrible\\\" appeared in positive reviews: \", positiveOccurances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building a basic sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #Vectorize our text with top 1000 frequent vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vectorizer and transform our data\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True) #Tokenize at word level\n",
    "vectorizer.fit(alltrain.data)\n",
    "train_data_vectorized = vectorizer.transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a logistic linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(train_data_vectorized, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  = 85.96%\n",
      "Average Precision = 85.97%\n",
      "Average Recall    = 85.96%\n",
      "Average F1-score  = 85.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "test_pred = model.predict(vectorizer.transform(test.data))\n",
    "#Preliminary evaluation with sklearn\n",
    "print (\"Accuracy  = %3.2f%%\" % (100*accuracy_score(test.target, test_pred)))\n",
    "print (\"Average Precision = %3.2f%%\" % (100*precision_score(test.target, test_pred, average='macro')))\n",
    "print (\"Average Recall    = %3.2f%%\" % (100*recall_score(test.target, test_pred, average='macro')))\n",
    "print (\"Average F1-score  = %3.2f%%\" % (100*f1_score(test.target, test_pred, average='macro')))\n",
    "# The scores for accuracy, and average precision, recall and F1-score are similar. \n",
    "# This is in part an effect of the balanced nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluating Accuracy, Recall and F1 scores without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances 25000\n",
      "Total correct predictions 21489\n",
      "Correctly predicted negative 10617\n",
      "Incorrectly predicted negative 1883\n",
      "Correctly predicted positive 10872\n",
      "Incorrectly predicted positive 1628\n"
     ]
    }
   ],
   "source": [
    "#Record the instances of correct and incorrect predictions\n",
    "num_neg = 0\n",
    "num_pos = 0\n",
    "neg_correct = 0\n",
    "neg_incorrect = 0\n",
    "pos_correct = 0\n",
    "pos_incorrect = 0\n",
    "\n",
    "for i in range(len(test.target)):\n",
    "    if test.target[i] == 0:\n",
    "        num_neg +=1\n",
    "        if test_pred[i] == 0:\n",
    "            neg_correct += 1\n",
    "        elif test_pred[i] == 1:\n",
    "            neg_incorrect += 1\n",
    "    elif test.target[i] == 1:\n",
    "        num_pos += 1\n",
    "        if test_pred[i] == 0:\n",
    "            pos_incorrect += 1\n",
    "        elif test_pred[i] == 1:\n",
    "            pos_correct += 1\n",
    "\n",
    "print(\"Total instances %d\" % (num_neg + num_pos))\n",
    "print(\"Total correct predictions %d\" % (pos_correct + neg_correct) )\n",
    "print(\"Correctly predicted negative %d\" % (neg_correct))\n",
    "print(\"Incorrectly predicted negative %d\" % (neg_incorrect))\n",
    "print(\"Correctly predicted positive %d\" % (pos_correct))\n",
    "print(\"Incorrectly predicted positive %d\" % (pos_incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.96%\n",
      "\n",
      "Neg label precision: 86.70%\n",
      "Neg label recall: 84.94%\n",
      "Neg F1 score: 85.81%\n",
      "\n",
      "Pos label precision: 85.24%\n",
      "Pos label recall: 86.98%\n",
      "Pos F1 score: 86.10%\n",
      "\n",
      "Average precision 85.97\n",
      "Average recall 85.96\n",
      "Average F1 score 85.95%\n"
     ]
    }
   ],
   "source": [
    "#Accuracy = Correctly predicted / Total\n",
    "Accuracy = (pos_correct + neg_correct)/(num_neg + num_pos)\n",
    "print(\"Accuracy: %2.2f%%\" % (100*Accuracy))\n",
    "print()\n",
    "\n",
    "#Neg label precision = neg_correct / (neg_correct + pos_incorrect)\n",
    "neg_precision = neg_correct / (neg_correct + pos_incorrect)\n",
    "print(\"Neg label precision: %2.2f%%\" % (100*neg_precision))\n",
    "\n",
    "#Recall of Neg = neg_correct / num_neg\n",
    "neg_recall = neg_correct / num_neg\n",
    "print(\"Neg label recall: %2.2f%%\" % (100*neg_recall))\n",
    "\n",
    "#F1-score harmonic mean of precision and recall. = \n",
    "neg_f1 = 1/( ((1/neg_precision)+(1/neg_recall))/2 )\n",
    "print(\"Neg F1 score: %2.2f%%\" % (100*neg_f1))\n",
    "print()\n",
    "\n",
    "#Pos label precision = pos_correct / (pos_correct + neg_incorrect)\n",
    "pos_precision = pos_correct / (pos_correct + neg_incorrect)\n",
    "print(\"Pos label precision: %2.2f%%\" % (100*pos_precision))\n",
    "\n",
    "#Recall of Pos = pos_correct / num_pos\n",
    "pos_recall = pos_correct / num_pos\n",
    "print(\"Pos label recall: %2.2f%%\" % (100*pos_recall))\n",
    "\n",
    "#F1-score harmonic mean of precision and recall. = \n",
    "pos_f1 = 1/( ((1/pos_precision)+(1/pos_recall))/2 )\n",
    "print(\"Pos F1 score: %2.2f%%\" % (100*pos_f1))\n",
    "print()\n",
    "\n",
    "print(\"Average precision %2.2f\" % (100*(neg_precision + pos_precision)/2))\n",
    "print(\"Average recall %2.2f\" % (100*(neg_recall + pos_recall)/2))\n",
    "print(\"Average F1 score %2.2f%%\" % (100*(neg_f1 + pos_f1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Individual accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Accuracy for each rating (1-4, 7-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating 1 accuracy: 91.48\n",
      "Rating 2 accuracy: 87.62\n",
      "Rating 3 accuracy: 81.74\n",
      "Rating 4 accuracy: 73.21\n",
      "\n",
      "Rating 7 accuracy: 79.11\n",
      "Rating 8 accuracy: 85.40\n",
      "Rating 9 accuracy: 89.46\n",
      "Rating 10 accuracy: 90.34\n"
     ]
    }
   ],
   "source": [
    "ratingCorrect = dict()\n",
    "ratingIncorrect = dict()\n",
    "ratings = [1,2,3,4,7,8,9,10]\n",
    "#Initialize the ratings\n",
    "for rating in ratings:\n",
    "    ratingCorrect[rating] = 0\n",
    "    ratingIncorrect[rating] = 0\n",
    "    \n",
    "#Count correct and incorrect instances\n",
    "for i in range(len(test.target)):\n",
    "    rating = getRating(test.filenames[i])\n",
    "    if test.target[i] == test_pred[i]:\n",
    "        ratingCorrect[rating] += 1\n",
    "    else:\n",
    "        ratingIncorrect[rating] += 1\n",
    "\n",
    "# Print the accuracy.\n",
    "for rating in ratings:\n",
    "    print(\"Rating %d accuracy: %2.2f\" % (rating, 100*ratingCorrect[rating]/(ratingCorrect[rating]+ratingIncorrect[rating])))\n",
    "    if rating == 4:\n",
    "        print() #Print a space betwen \"positive\" and \"negative\" ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our performance increases near the extreme for the negative class (1) and the extreme for the positive class (10). The performance decreases when the ratings are more neural and close to the center of the scale. This was predicted earlier in ยง 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data samples for each instance in the cofusion matrix (TP FP FN TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze instances of TN(Neg Neg) FP (Neg Pos) FN(Pos Neg) and TP (Pos Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInstance(gold, pred):\n",
    "    for i in range(len(test.target)):\n",
    "        if test.target[i] == gold and test_pred[i] == pred:\n",
    "            return(i)\n",
    "    return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative\n",
      "Gold: Neg | Predicted: Neg | Match: Correct | Rating:  1\n",
      "Text:\n",
      "b'I don\\'t know how this movie has received so many positive comments. One can call it \"artistic\" and \"beautifully filmed\", but those things don\\'t make up for the empty plot that was filled with sexual innuendos. I wish I had not wasted my time to watch this movie. Rather than being biographical, it was a poor excuse for promoting strange and lewd behavior. It was just another Hollywood attempt to convince us that that kind of life is normal and OK. From the very beginning I asked my self what was the point of this movie,and I continued watching, hoping that it would change and was quite disappointed that it continued in the same vein. I am so glad I did not spend the money to see this in a theater!'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"True negative\")\n",
    "instance = getInstance(0,0)\n",
    "print(\"Gold: Neg | Predicted: Neg | Match: Correct | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This negative example was correctly predicted negatively. Since we are analyzing word frequency, this review has a lot of words such as \"dissapointed\" \"poor\" \"attempt\" and \"wasted\" which would have likely pushed the algorithm to predict a negative classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive\n",
      "Gold: Neg | Predicted: Pos | Match: Incorrect | Rating:  4\n",
      "Text:\n",
      "b\"Even Disney are guilty of the cash cow disease, after the roaring success of The Love Bug in 1968, the house of mouse cashed in with Herbie Rides Again, Herbie Goes To Monte Carlo, and Herbie Goes Bananas. Neither sequel capturing the charm and inoffensive appeal of The Love Bug back in 68, in this one we find race driver Jim Douglas and his sidekick Wheely Applegate, entering Herbie in the Monte Carlo Rally. Naturally things outside of the race start to take over priorities, they get mixed up in a diamond robbery and Herbie falls in love with another car!. The car stunts are of course pleasant and easy on the eye, and it would be churlish of me to really vent venom on such a friendly piece of fluff, it's just that the film goes nowhere fast and personally now i can see it for the coin motivated piece of work it is. Still you get to see Herbie take a bath, foil the baddies and of course dance for the lady in his life, so something there for everyone i think....................4/10.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"False positive\")\n",
    "instance = getInstance(0,1)\n",
    "print(\"Gold: Neg | Predicted: Pos | Match: Incorrect | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example was predicted to be positive when it was negative. First this example was a difficult review to classify as it's rating was quite near the center (4). Second, the review uses a lot of vocabulary such as \"love\" \"pleasant\" \"charm\" frequently which indicate that the reviewed liked the movie. The model doesn't know that the reviewer could be describing the contents of the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negative\n",
      "Gold: Pos | Predicted: Neg | Match: Incorrect | Rating:  7\n",
      "Text:\n",
      "b\"The casting of Robert Culp is probably the only decent move the production team made with this film. Falk and Culp were marvellous, but as culp was not Falks nemesis this time, chemistry was lacking. Columbo is only as strong as his opposite number, and this time he didn't have one.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"False negative\")\n",
    "instance = getInstance(1,0)\n",
    "print(\"Gold: Pos | Predicted: Neg | Match: Incorrect | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the above review, this review was also close to the center of the scale. In addition, even as a human reading the review, it would be difficult to predict the correct rating. The words used in this review doesn't indicate clearly that it is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive\n",
      "Gold: Pos | Predicted: Pos | Match: Correct | Rating:  9\n",
      "Text:\n",
      "b\"Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\"\n"
     ]
    }
   ],
   "source": [
    "print(\"True positive\")\n",
    "instance = getInstance(1,1)\n",
    "print(\"Gold: Pos | Predicted: Pos | Match: Correct | Rating: \", getRating(test.filenames[instance]))\n",
    "print(\"Text:\")\n",
    "print(test.data[instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was correctly predicted positive. It's likely because of the positive words used \"fun\", \"work well\" and \"congratulations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Investigating the effect of training size on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We increment the training size from 1000 to 25000 in increments of 1000 and examine the increase in accuracy as we increase the training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000,analyzer='word',lowercase=True) #Tokenize at word level\n",
    "vectorizer.fit(alltrain.data) #We use all the data for training the vectorization strategy as this is not part of the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1000 Average F1-score: 77.57%\n",
      "Training size: 2000 Average F1-score: 79.22%\n",
      "Training size: 3000 Average F1-score: 79.96%\n",
      "Training size: 4000 Average F1-score: 80.56%\n",
      "Training size: 5000 Average F1-score: 81.76%\n",
      "Training size: 6000 Average F1-score: 82.57%\n",
      "Training size: 7000 Average F1-score: 82.83%\n",
      "Training size: 8000 Average F1-score: 83.71%\n",
      "Training size: 9000 Average F1-score: 84.24%\n",
      "Training size: 10000 Average F1-score: 84.52%\n",
      "Training size: 11000 Average F1-score: 84.58%\n",
      "Training size: 12000 Average F1-score: 84.76%\n",
      "Training size: 13000 Average F1-score: 84.97%\n",
      "Training size: 14000 Average F1-score: 85.18%\n",
      "Training size: 15000 Average F1-score: 85.33%\n",
      "Training size: 16000 Average F1-score: 85.59%\n",
      "Training size: 17000 Average F1-score: 85.51%\n",
      "Training size: 18000 Average F1-score: 85.68%\n",
      "Training size: 19000 Average F1-score: 85.70%\n",
      "Training size: 20000 Average F1-score: 85.72%\n",
      "Training size: 21000 Average F1-score: 85.82%\n",
      "Training size: 22000 Average F1-score: 85.91%\n",
      "Training size: 23000 Average F1-score: 86.08%\n",
      "Training size: 24000 Average F1-score: 86.02%\n",
      "Training size: 25000 Average F1-score: 85.95%\n"
     ]
    }
   ],
   "source": [
    "#Train the model with varying sizes of train data\n",
    "F1list = [] #Track the F1 scores\n",
    "trainSize = [] #Track our training size\n",
    "for i in range(1,26):\n",
    "    trainSize.append(i*1000)\n",
    "    #Create a model\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    #Train with our dataset\n",
    "    model.fit(vectorizer.transform(train.data[0:i*1000]), train.target[0:i*1000])\n",
    "    #Vectorize our test data.\n",
    "    test_pred = model.predict(vectorizer.transform(test.data))\n",
    "    #Determine our F1 score\n",
    "    F1 = 100*f1_score(test.target, test_pred, average='macro')\n",
    "    print (\"Training size: %d Average F1-score: %3.2f%%\" % (i*1000, F1))\n",
    "    F1list.append(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot our F1 against data Size\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.plot(trainSize,F1list,color='blue')\n",
    "plt.xlabel('Training size', fontsize=10)\n",
    "plt.ylabel('F-score %', fontsize=10)\n",
    "plt.title('Learning Curve',fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I observe that our logistic regression model was suprisingly accurate even when training even with a minimal of 1000 samples. It seems that given only 1000 samples, our model could identify the sentiment relatively well\n",
    "\n",
    "I observe that as the training size increases, the F1 score of the model increases. For example, with only 1000 training samples we can achieve an accuracy of 77.6% while with 5,000 training samples we can achieve an accuracy of 81.8%. This is to be expected because the more training examples we have, the better the predicting capability and generalization to the unseen test the model is.\n",
    "\n",
    "I also observe that as the training size increases, the rate of gain in F1 of adding on an additional 1000 sample decreased. Simply put the F1 score did not increase linearlly to the size of the Training. There appears to be a log relationship.\n",
    "\n",
    "Between 1000 and 2000 examples, the F score increased 1.65% Between 2000 and 4000 it increased 1.34%. Between 4000 and 8000 it increased 3.15%. Between 8000 and 16000 it increased 1.88%. \n",
    "\n",
    "Following this observed relationship, I would expect an increase of around 2% for every doubling in training size. Hence, I expect an increase of another 2% to 88% if we add another 25000 examples. (Although this is not guranteed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
